{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96148de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from reV.SAM.generation import WindPower\n",
    "import PySAM.Windpower as Windpower\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sqlite3\n",
    "from rex import Resource\n",
    "from reV.losses.power_curve import (\n",
    "    PowerCurve,\n",
    "    PowerCurveLossesInput,\n",
    "    PowerCurveWindResource,\n",
    "    adjust_power_curve,\n",
    ")\n",
    "from pyproj import Proj, Transformer\n",
    "import ast\n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "\n",
    "def flatten_sam_dict(nested_dict):\n",
    "    flat = {}\n",
    "    for group in nested_dict.values():\n",
    "        flat.update(group)\n",
    "    return flat\n",
    "\n",
    "db_powercurves_path = r'C:\\Users\\mouc074\\OneDrive - PNNL\\Documents\\wind_energy_research\\wind_data\\powercurves_table.csv'\n",
    "db_powercurves = pd.read_csv(db_powercurves_path)\n",
    "\n",
    "plant_data_path =r\"C:\\Users\\mouc074\\OneDrive - PNNL\\Documents\\wind_energy_research\\wind_data\\generators_table.csv\"\n",
    "windplant_database = pd.read_csv(plant_data_path)\n",
    "\n",
    "db_turbine_path = r'C:\\Users\\mouc074\\OneDrive - PNNL\\Documents\\wind_energy_research\\wind_data\\turbine_byplant_table.csv'\n",
    "db_turbine = pd.read_csv(db_turbine_path)\n",
    "\n",
    "data_folder = r\"C:\\Users\\mouc074\\OneDrive - PNNL\\Documents\\wind_energy_research\\wind_data\\HRRR_data\\interpolated\"\n",
    "db_path = r\"\\\\pnl\\projects\\Wind_Power_Data\\Task 2\\Task 2.1\\Database\\winddata_v11.db\"   \n",
    "\n",
    "turbine_updated_names = r\"C:\\Users\\mouc074\\OneDrive - PNNL\\Documents\\wind_energy_research\\wind_data\\turbines_Powercurve_mismatch.csv\"\n",
    "updated_names_df= pd.read_csv(turbine_updated_names, encoding=\"latin1\")\n",
    "\n",
    "\n",
    "# Default Power curve parameters\n",
    "default_turbine_model = 'generic_A_1'\n",
    "default_turbine_manufacturer = 'generic'\n",
    "generic_turbing_hheight = 80 # meters\n",
    "generic_turbine_rotor_diameter = 97 # meters\n",
    "\n",
    "#Powercurve source specification \n",
    "preferred_sources = ['thewindpower.net','wind turbines db']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plant_ids = [56291]\n",
    "#plant_ids = [56322,56270,56322,56457,56649,56774,57192,57050,57192,58441,56394,56644,56673,56773,57049,57188,57530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing turbine powercurve and mismatch\n",
    "\n",
    "# Merge to align matches\n",
    "merged = windplant_database.merge(\n",
    "    updated_names_df,\n",
    "    left_on=['major_t_manu', 'major_t_model'],\n",
    "    right_on=['manufacturer', 'model'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Update only where we have a match AND no \"Missing\"\n",
    "merged['major_t_manu'] = merged.apply(\n",
    "    lambda r: r['new_manufacturer'] if pd.notna(r['new_manufacturer']) and r['new_manufacturer'] != 'Missing' else r['major_t_manu'],\n",
    "    axis=1\n",
    ")\n",
    "merged['major_t_model'] = merged.apply(\n",
    "    lambda r: r['new_model'] if pd.notna(r['new_model']) and r['new_model'] != 'Missing' else r['major_t_model'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only original df1 columns (now updated)\n",
    "windplant_database_updated = merged[windplant_database.columns]\n",
    "\n",
    "print(windplant_database_updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b5fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sources = db_powercurves['source'].unique()\n",
    "print(unique_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bdddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = r\"\\\\pnl\\projects\\Wind_Power_Data\\Task 2\\Task 2.1\\Database\\winddata_v11.db\"   # Windows\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# load table into dataframe\n",
    "windplant_database = pd.read_sql_query(\"SELECT * FROM generators\", conn)\n",
    "\n",
    "\n",
    "powercurves = pd.read_sql_query(\"SELECT * FROM power_curve\", conn)\n",
    "\n",
    "db_powercurves = powercurves.groupby(['manufacturer','model','source']).agg({\n",
    "    'wind_spd_ms': list,\n",
    "    'power_kw': list,\n",
    "}).reset_index()\n",
    "\n",
    "turbines =pd.read_sql_query(\"SELECT * FROM turbines\", conn)\n",
    "\n",
    "db_turbines = turbines.groupby(['plant_id','turbine_model','project_name']).agg({\n",
    "    'latitude': list,\n",
    "    'longitude': list,\n",
    "    'p_tnum':'first',\n",
    "    'turbine_manufacturer':'first',\n",
    "    'turbine_hub_height':'first',\n",
    "    'turbine_rotor_diameter':'first',\n",
    "    'turbine_total_height':'first',\n",
    "    'power_diameter':'first'\n",
    "}).reset_index()\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87651be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check Verison of PySAM(recommended 6.0.1)\n",
    "import PySAM\n",
    "print(PySAM.__version__)\n",
    "\n",
    "# Check Verison of reV(recommended 0.13.1)\n",
    "import reV\n",
    "print(reV.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b931233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf = TimezoneFinder()\n",
    "\n",
    "def get_utc_offset(lat, lon):\n",
    "    \"\"\"Return UTC offset string like 'UTC-7' from lat/lon\"\"\"\n",
    "    tz_name = tf.timezone_at(lat=lat, lng=lon)\n",
    "    if not tz_name:\n",
    "        return None\n",
    "    tz = pytz.timezone(tz_name)\n",
    "    now = datetime.now(tz)\n",
    "    offset_hours = now.utcoffset().total_seconds() / 3600\n",
    "    return f\"{offset_hours:+.0f}\"\n",
    "\n",
    "def filter_and_assign(df, value):\n",
    "    \"\"\"\n",
    "    Return all rows where df['name'] == value.\n",
    "    \"\"\"\n",
    "    filtered_df = df[df['plant_id'] == value].copy()\n",
    "\n",
    "    # Add UTC offset column\n",
    "    filtered_df[\"UTC_offset\"] = filtered_df.apply(\n",
    "        lambda row: get_utc_offset(row[\"latitude\"], row[\"longitude\"]), axis=1\n",
    "    )\n",
    "    filtered_df[\"elevation\"] = 0 \n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def combine_lists(x):\n",
    "    return [item for sublist in x for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = '4'\n",
    "for plant_id in plant_ids:\n",
    "    new_plant_df =filter_and_assign(windplant_database_updated, plant_id)\n",
    "    group_col = \"major_t_model\"     # column to group by\n",
    "    sum_cols = [\"nameplate_capacity_mw\",\"n_turbines\"]       # numeric columns to sum\n",
    "\n",
    "    #  Coerce numeric columns \n",
    "    for c in sum_cols:\n",
    "        new_plant_df[c] = pd.to_numeric(\n",
    "            new_plant_df[c].astype(str).str.replace(',', '').str.strip(),\n",
    "            errors='coerce'\n",
    "        ).fillna(0)\n",
    "\n",
    "    #  Identify non-sum  columns \n",
    "    first_cols = [c for c in new_plant_df.columns if c not in sum_cols]\n",
    "\n",
    "    # --- 3) Group by the turbine type within plant and combine capacity and number of turbines---\n",
    "    combined_df = (\n",
    "        new_plant_df.groupby(group_col, as_index=False)\n",
    "            .agg({**{c: 'first' for c in first_cols}, **{c: 'sum' for c in sum_cols}})\n",
    "    )\n",
    "\n",
    "    for c in sum_cols:\n",
    "        new_plant_df[c] = pd.to_numeric(\n",
    "            new_plant_df[c].astype(str).str.replace(',', '').str.strip(),\n",
    "            errors='coerce'\n",
    "        ).fillna(0)\n",
    "\n",
    "\n",
    "    # Have to specify within the turbine \n",
    "    row = db_turbine[db_turbine['plant_id'] == plant_id]\n",
    "    row['longitude'] = row['longitude'].apply(ast.literal_eval)\n",
    "    row['latitude'] = row['latitude'].apply(ast.literal_eval)\n",
    "    row['num_items']= row['longitude'].apply(len)\n",
    "\n",
    "    combined_turbine_row = (\n",
    "        row.groupby('turbine_model', sort=False)\n",
    "        .agg({\n",
    "            'latitude': combine_lists,\n",
    "            'longitude': combine_lists,\n",
    "            'plant_id': 'first',\n",
    "            'turbine_manufacturer':'first',\n",
    "            'turbine_rated_power':'first',\n",
    "            'turbine_hub_height':'first',\n",
    "            'turbine_rotor_diameter':'first',\n",
    "            'turbine_total_height':'first',\n",
    "            'power_diameter':'first',\n",
    "            'num_items':'sum'\n",
    "\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    number = str(plant_id)  # convert to string just in case\n",
    "    print(f\"Processing {number}\")\n",
    "\n",
    "    # Find all files in the folder that contain this number\n",
    "    matching_files = [f for f in os.listdir(data_folder) if number in f and f.endswith(\".csv\")]\n",
    "\n",
    "    if not matching_files:\n",
    "        print(f\"⚠️ No CSV found containing '{number}'\")\n",
    "        continue\n",
    "\n",
    "    if len(matching_files) > 1:\n",
    "        print(f\"Multiple files found for '{number}': {matching_files}\")\n",
    "        # you can pick the first one or add logic to choose\n",
    "        csv_file = matching_files[0]\n",
    "    else:\n",
    "        csv_file = matching_files[0]\n",
    "\n",
    "    csv_path = os.path.join(data_folder, csv_file)\n",
    "    print(f\"✅ Loading CSV: {csv_path}\")\n",
    "\n",
    "    # Load timeseries CSV\n",
    "    ts_df = pd.read_csv(csv_path)\n",
    "    data_df = ts_df\n",
    "\n",
    "    data_df['datetime'] = pd.to_datetime(data_df['valid_time'])\n",
    "    data_df.set_index('datetime', inplace=True)\n",
    "\n",
    "    # Lapse rate and height\n",
    "    lapse_rate = 0.0065  # °C/m or K/m\n",
    "\n",
    "    surface_pressure = data_df['pressure_Pa']  \n",
    "    temperature = data_df['temperature_K'] \n",
    "\n",
    "    # change with height of turbine\n",
    "    height_80 = 80\n",
    "    height_140 = 140\n",
    "\n",
    "    g = 9.80665\n",
    "    R = 287.05\n",
    "\n",
    "    #Find Pressure at 80m and 140m above surface level\n",
    "    pressure_80m = surface_pressure * np.exp(-g * height_80 / (R * temperature))\n",
    "    pressure_140m = surface_pressure * np.exp(-g * height_140 / (R * temperature))\n",
    "\n",
    "    data_df['Pressure_80m'] = (pressure_80m) \n",
    "    data_df['Pressure_140m'] = pressure_140m\n",
    "    data_df['temperature'] = data_df['temperature_K'] -273.15\n",
    "\n",
    "    data_df['datetime'] = pd.to_datetime(data_df['valid_time'])\n",
    "    data_df.set_index('datetime', inplace=True)\n",
    "\n",
    "    data_df.rename(columns={'Pressure_80m': 'pressure', 'wind_speed': 'windspeed', 'wind_dir': 'winddirection'}, inplace=True)\n",
    "    \n",
    "    #Initialize values for plants with multiple sites\n",
    "    total_generation = pd.Series(0, index=range(8760))\n",
    "    total_plant_capacity = 0\n",
    "    #iterate over turbine type\n",
    "    for idx, row in combined_df.iterrows():\n",
    "        single_df = pd.DataFrame([row])\n",
    "        plant_capacity = single_df['nameplate_capacity_mw']*1000 # convert to kw\n",
    "        \n",
    "        #create meta data\n",
    "        meta_df = pd.DataFrame()\n",
    "        meta_df['latitude'] = single_df['latitude']\n",
    "        meta_df['longitude'] = single_df['longitude']\n",
    "        meta_df['elevation'] = single_df['elevation']\n",
    "        meta_df['timezone'] = single_df['UTC_offset'].astype(int)\n",
    "  \n",
    "        # pull turbine locations(need to alter to be able to handle multiple sites)\n",
    "        #row = db_turbine[db_turbine['plant_id'] == plant_id ]\n",
    "        turbine_row_filtered = combined_turbine_row[combined_turbine_row[\"num_items\"].isin(single_df[\"n_turbines\"])]\n",
    "\n",
    "        x_coords = turbine_row_filtered['longitude'].iloc[0]\n",
    "        y_coords = turbine_row_filtered['latitude'].iloc[0]\n",
    "        # Use transformer to change lat/long into (x,y) coordinates\n",
    "        transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:9311\", always_xy=True)  # WGS84 to UTM zone 14N\n",
    "        lon_list, lat_list = transformer.transform(x_coords, y_coords)\n",
    "\n",
    "        # Shift so first turbine is at (0, 0)\n",
    "        x_coords = np.array(lon_list) - lon_list[0]\n",
    "        y_coords = np.array(lat_list) - lat_list[0]\n",
    "\n",
    "        #collect turbine data\n",
    "        turbine_model = single_df['major_t_model'].iloc[0]\n",
    "        turbine_manufacturer = single_df['major_t_manu'].iloc[0]\n",
    "\n",
    "        #collect powercurve data\n",
    "        model_row = db_powercurves[(db_powercurves['model'] ==turbine_model) & (db_powercurves['manufacturer'] == turbine_manufacturer) & (db_powercurves['source'].isin(preferred_sources)) ] \n",
    "        \n",
    "        # if perferred powercuve source not available, use other source\n",
    "        if model_row.empty:\n",
    "            row = db_powercurves[\n",
    "                (db_powercurves['model'] == turbine_model) &\n",
    "                (db_powercurves['manufacturer'] == turbine_manufacturer)\n",
    "            ]\n",
    "\n",
    "        #Check if turbine powercurve available, if not use generic \n",
    "        used_default = False\n",
    "        if model_row.empty:\n",
    "            model_row = db_powercurves[(db_powercurves['model'] == default_turbine_model) & (db_powercurves['manufacturer'] == default_turbine_manufacturer)]\n",
    "            used_default = True\n",
    "            \n",
    "        if used_default:\n",
    "            rotor_diameter = generic_turbine_rotor_diameter\n",
    "            hub_height = generic_turbing_hheight   \n",
    "        else:\n",
    "            rotor_diameter = turbine_row_filtered['turbine_rotor_diameter'].iloc[0]\n",
    "            hub_height = turbine_row_filtered['turbine_hub_height'].iloc[0]\n",
    "\n",
    "        wind_speed = model_row['wind_spd_ms'].iloc[0]\n",
    "        power_out = model_row['power_kw'].iloc[0]\n",
    "\n",
    "        temperatures = data_df['temperature'] \n",
    "        pressures = data_df['pressure']\n",
    "        wind_speeds = data_df['windspeed']\n",
    "\n",
    "        #power curve losses\n",
    "        wind_speed = ast.literal_eval(wind_speed)\n",
    "        power_out = ast.literal_eval(power_out)\n",
    "        pc_wind_speed = pd.Series(wind_speed)\n",
    "        pc_generation = pd.Series(power_out)\n",
    "\n",
    "        # have to incorporate turbine powercurve scaling if using the generic power curve\n",
    "        if used_default:\n",
    "            rev_capacity = x_coords.size * pc_generation.iloc[-1]\n",
    "            power_curve_scaling_factor = plant_capacity.iloc[0]/rev_capacity\n",
    "            pc_generation = [x * power_curve_scaling_factor for x in pc_generation]\n",
    "\n",
    "        power_curve_loss_info = {\n",
    "            'target_losses_percent': 5,\n",
    "            'transformation': 'exponential_stretching'\n",
    "        }\n",
    "\n",
    "        power_curve = PowerCurve(pc_wind_speed, pc_generation)\n",
    "        resource_data = PowerCurveWindResource(temperatures, pressures, wind_speeds)\n",
    "        target_losses = PowerCurveLossesInput(power_curve_loss_info)\n",
    "\n",
    "        new_curve = adjust_power_curve(\n",
    "            power_curve, resource_data, target_losses\n",
    "        )\n",
    "        _ = plt.plot(power_curve.wind_speed, power_curve, label='Original')\n",
    "        _ = plt.plot(new_curve.wind_speed, new_curve.generation, label='11% Losses')\n",
    "        _ = plt.legend(loc='upper left')\n",
    "        _ = plt.xlabel(\"Wind Speed (m/s)\")\n",
    "        _ = plt.ylabel(\"Generated Power (kW)\")\n",
    "        _ = plt.show()\n",
    "\n",
    "\n",
    "        windspeeds_list = new_curve.wind_speed.tolist()\n",
    "        powerout_list = new_curve.generation.tolist()\n",
    "\n",
    "        #create wind resource dictionary \n",
    "        ordered_fields = ['temperature', 'pressure','windspeed', 'winddirection']\n",
    "        field_ids = [1, 2, 3, 4]\n",
    "        heights = [80, 80, 80, 80]  # change to match hub height\n",
    "\n",
    "        df_subset = data_df[ordered_fields].copy().dropna()\n",
    "        data_array = df_subset[ordered_fields].values.tolist()\n",
    "\n",
    "        wind_resource_dict = {\n",
    "            'latitude':  float(meta_df['latitude'].iloc[0]),\n",
    "            'longitude': float(meta_df['longitude'].iloc[0]),\n",
    "            'elevation' : float(meta_df['elevation'].iloc[0]),\n",
    "            'year': 2018,\n",
    "            'fields': field_ids,\n",
    "            'heights': heights,\n",
    "            'data': data_array,\n",
    "            'rh' : data_df['rh'].values.tolist()\n",
    "        }\n",
    "        print(wind_resource_dict)\n",
    "\n",
    "        # create wind configuration with specified parameters\n",
    "        wind_config = Windpower.new()\n",
    "        wind_config.Turbine.wind_turbine_hub_ht = hub_height\n",
    "        wind_config.Turbine.wind_turbine_rotor_diameter = rotor_diameter #97 A1\n",
    "        wind_config.Resource.wind_resource_model_choice = 0 #hourly\n",
    "        wind_config.Turbine.wind_turbine_powercurve_windspeeds = windspeeds_list\n",
    "        wind_config.Turbine.wind_turbine_powercurve_powerout = powerout_list\n",
    "        wind_config.Losses.turb_generic_loss =0\n",
    "        wind_config.Losses.wake_int_loss = 0\n",
    "\n",
    "        #icing stuff\n",
    "        wind_config.Losses.en_icing_cutoff = 1\n",
    "        wind_config.Losses.icing_cutoff_temp = -18\n",
    "        wind_config.Losses.icing_cutoff_rh = 95\n",
    "        wind_config.Losses.low_temp_cutoff = -29\n",
    "\n",
    "        wind_config.Farm.wind_farm_wake_model = 1 #0-3 simple, park, ev,constant\n",
    "        wind_config.Farm.system_capacity = plant_capacity.iloc[0]#Spring Valley\n",
    "        wind_config.Farm.wind_resource_turbulence_coeff = 0.10 # remove\n",
    "        wind_config.Farm.wind_farm_xCoordinates = x_coords\n",
    "        wind_config.Farm.wind_farm_yCoordinates = y_coords\n",
    "        wind_config.Farm.wake_loss_multiplier = 1.5 #value greater than means increased wake losses, value less than 1 means decreased wake losses, cannot be assigned for constant model\n",
    "        wind_config.Resource.wind_resource_data = wind_resource_dict\n",
    "        wind_config.Farm.park_wake_decay_constant = 0.075 #0.05-0.075\n",
    "        wind_config.Turbine.wind_resource_shear = 0.15 #remove? # shouldnt effect anything because we have wind speed at hub height\n",
    "        wind_config.execute()\n",
    "\n",
    "\n",
    "        # Access PySAM results\n",
    "        gen_profile_PySAM = wind_config.Outputs.gen\n",
    "        capacity_factor_PySAM = tuple(x / plant_capacity  for x in gen_profile_PySAM)\n",
    "        PySAM_CF = pd.DataFrame(capacity_factor_PySAM, columns=['Capacity Factor'])\n",
    "        wind_speeds_PySAM = wind_config.Outputs.wind_speed\n",
    "\n",
    "        # Run reV\n",
    "        nested_dict = wind_config.export()\n",
    "        flat_dict= flatten_sam_dict(nested_dict)\n",
    "        with open(\"sam_inputs_flat.json\", \"w\") as f:\n",
    "            json.dump(flat_dict, f, indent=2)\n",
    "            #Set Parameters\n",
    "        output_request = ['gen_profile','capacity_factor','wind_speed', 'cf_profile']\n",
    "        wind_simulation = WindPower(\n",
    "            resource  = data_df[['windspeed', 'temperature', 'pressure','winddirection','rh']], \n",
    "            meta = meta_df,\n",
    "            sam_sys_inputs = flat_dict,\n",
    "            output_request = output_request\n",
    "        )\n",
    "        wind_simulation.run()\n",
    "        outputs = wind_simulation.outputs\n",
    "        wind_speed_reV_SAM_1 = outputs['wind_speed']\n",
    "        gen_profile_reV_SAM_1 = outputs['gen_profile']\n",
    "        cf_profile_reV_SAM_1 = outputs['cf_profile']\n",
    "        plant_capacity_scalar = plant_capacity.iloc[0]\n",
    "\n",
    "        df_combined_reV = pd.DataFrame({\n",
    "            'datetime': data_df.index,\n",
    "            'cf_reV': cf_profile_reV_SAM_1,\n",
    "            'gen_reV' : gen_profile_reV_SAM_1,\n",
    "            'gen_SAM' : gen_profile_PySAM\n",
    "        })\n",
    "\n",
    "        #Remove negative values and values that exceed plant capacity \n",
    "        df_combined_reV['cf_reV'] = df_combined_reV['cf_reV'].clip(lower=0, upper=1)\n",
    "        df_combined_reV['gen_reV'] = df_combined_reV['gen_reV'].clip(lower=0, upper=plant_capacity_scalar)\n",
    "\n",
    "        # get total generation and capacity among all the plant's sites/expansions\n",
    "        plant_capacity_scalar = plant_capacity.iloc[0]  # or num.values[0]\n",
    "        total_generation += df_combined_reV['gen_reV']\n",
    "        total_plant_capacity  += plant_capacity_scalar\n",
    "\n",
    "        #Export Profiles to csv\n",
    "        dire = f'./rev_HRRR_data/{plant_id}'\n",
    "        os.makedirs(dire, exist_ok=True)\n",
    "        safe_model = str(turbine_model).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "        save_path = (os.path.join(dire, f\"{plant_id}_{safe_model}_{run_number}.csv\")) \n",
    "        df_combined_reV.to_csv(save_path, index=False)\n",
    "    \n",
    "    #\n",
    "    total_capacity_factor = total_generation/total_plant_capacity\n",
    "    if len(combined_df) > 1:\n",
    "        df_total = pd.DataFrame({\n",
    "                'datetime': data_df.index,\n",
    "                'total_gen': total_generation,\n",
    "                'combined_capacity_factor': total_capacity_factor,\n",
    "        })\n",
    "\n",
    "        save_path = (os.path.join(dire, f\"{plant_id}_combined_{run_number}.csv\")) \n",
    "        df_total.to_csv(save_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysam7_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
